<div align="center">

# ğŸ§  DocuMind AI
### Production-Grade Multimodal RAG System

**Hybrid Search Â· Vision AI Â· Agentic Intelligence Â· LLM Evaluation**

[![Python](https://img.shields.io/badge/Python-3.10+-blue?style=flat-square)](https://python.org)
[![React](https://img.shields.io/badge/React-18-61DAFB?style=flat-square)](https://reactjs.org)
[![LangGraph](https://img.shields.io/badge/LangGraph-Agent-green?style=flat-square)](https://langchain-ai.github.io/langgraph)
[![Gemini](https://img.shields.io/badge/Gemini-Free_Tier-orange?style=flat-square)](https://aistudio.google.com)
[![License](https://img.shields.io/badge/License-MIT-purple?style=flat-square)](LICENSE)

[Features](#-features) Â· [Architecture](#-architecture) Â· [Setup](#-quick-setup) Â· [How It Works](#-how-it-works) Â· [API](#-api-reference)

</div>

---

## âœ¨ Features

- ğŸ”® **True Multimodal RAG** â€” Gemini Vision understands charts, diagrams, tables, and images in PDFs â€” not just text
- âš¡ **Hybrid Search** â€” Combines semantic vector search (all-MiniLM), BM25 keyword search, and CLIP image embeddings via Reciprocal Rank Fusion
- ğŸ¤– **Agentic Intelligence** â€” LangGraph agent analyzes each question, evaluates all search modes, selects the best answer, and self-refines if quality is low
- ğŸ“Š **LLM-as-Judge Evaluation** â€” Gemini scores answer faithfulness and completeness without needing ground truth
- ğŸŒ **External Factual Check** â€” Independent verification of answers against general knowledge
- ğŸ” **Consistency Testing** â€” Measures how stable answers are across repeated runs
- ğŸ’¡ **Winner Reasoning** â€” Explains *why* a particular search method won for each question type

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DocuMind AI                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Frontend   â”‚  React 18 Â· Geist fonts Â· Light theme            â”‚
â”‚  (port 3000) â”‚  Chat UI Â· Evaluation modals Â· Comparison view   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Flask API  (port 5000)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Document   â”‚   RAG Engine      â”‚   Agentic RAG                 â”‚
â”‚  Processor  â”‚                   â”‚   (LangGraph)                 â”‚
â”‚             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                               â”‚
â”‚ PyPDF2      â”‚  â”‚ ChromaDB    â”‚  â”‚  analyze_query                â”‚
â”‚  + Gemini   â”‚  â”‚ (vectors)   â”‚  â”‚    â†’ evaluate_all_modes       â”‚
â”‚    Vision   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚    â†’ select_best_mode         â”‚
â”‚  + Camelot  â”‚  â”‚ BM25 Index  â”‚  â”‚    â†’ evaluate_quality         â”‚
â”‚    Tables   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚    â†’ finalize_answer          â”‚
â”‚  + CLIP     â”‚  â”‚ CLIP Index  â”‚  â”‚                               â”‚
â”‚    Embeds   â”‚  â”‚ (512-dim)   â”‚  â”‚  Shared RAGEvaluator          â”‚
â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  (agent + dashboard agree)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Evaluation Layer                              â”‚
â”‚  Embedding similarity Â· LLM Judge Â· Factual Check Â· Consistency â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Setup

### Prerequisites
- Python 3.10+
- Node.js 18+
- [Gemini API key](https://aistudio.google.com/app/apikey) (free)
- Windows: [Poppler](https://github.com/oschwartz10612/poppler-windows/releases) for vision processing

### 1. Clone
```bash
git clone https://github.com/uXmii/DocuMind-AI.git
cd DocuMind-AI
```

### 2. Backend
```bash
cd backend
python -m venv venv
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

pip install -r requirements.txt
```

### 3. Environment variables
```bash
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY
```

### 4. Frontend
```bash
cd frontend
npm install
```

### 5. Run
```bash
# Terminal 1 â€” Backend
cd backend
python app.py

# Terminal 2 â€” Frontend
cd frontend
npm start
```

Open **http://localhost:3000** and upload a PDF to get started.

---

## ğŸ” How It Works

### ğŸ“„ Document Processing Pipeline

When you upload a PDF, it goes through three parallel processing tracks:

```
PDF Upload
    â”‚
    â”œâ”€â”€ 1. TEXT EXTRACTION (PyPDF2)
    â”‚       Sentence-aware chunking (1000 chars, 200 overlap)
    â”‚       â†’ text chunks with page metadata
    â”‚
    â”œâ”€â”€ 2. VISION PROCESSING (Gemini Vision)
    â”‚       Every page â†’ image â†’ Gemini describes it
    â”‚       Understands: charts, diagrams, tables, equations
    â”‚       â†’ vision chunks with rich semantic descriptions
    â”‚       â†’ CLIP image embeddings (512-dim) stored separately
    â”‚
    â””â”€â”€ 3. TABLE EXTRACTION (Camelot)
            Lattice & stream detection
            â†’ structured table chunks
```

**Why this matters:** A standard RAG system using PyPDF2 alone would completely miss the flowchart on page 4, the GDP timeline chart, and the Hundi comparison table. Vision processing makes these fully searchable.

---

### ğŸ” Hybrid Search (3-way Fusion)

Every query runs three searches simultaneously:

| Search Type | How it works | Best for |
|-------------|-------------|---------|
| **Semantic (Vector)** | `all-MiniLM-L6-v2` embeddings, cosine similarity in ChromaDB | Conceptual questions, paraphrased queries |
| **Keyword (BM25)** | Okapi BM25 statistical ranking, exact term matching | Specific names, dates, precise facts |
| **CLIP Visual** | Text query â†’ CLIP text encoder â†’ searches image embedding space | Questions about diagrams, charts, visual content |

Results are merged using **Reciprocal Rank Fusion (RRF)**:
```
score(doc) = Î£ 1 / (k + rank_in_method)   where k=60
```

This rewards documents that appear highly ranked in multiple search modes.

---

### ğŸ¤– Agentic RAG (LangGraph)

The agent follows a deterministic graph:

```
analyze_query
      â”‚
      â–¼
evaluate_all_modes  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                 â”‚
      â–¼                                 â”‚
select_best_mode                        â”‚
      â”‚                                 â”‚
      â–¼                                 â”‚
evaluate_quality â”€â”€[needs_refinement]â”€â”€â”˜
      â”‚
      [good_enough]
      â”‚
      â–¼
finalize_answer
```

**Step by step:**

1. **analyze_query** â€” Classifies question as factual/conceptual/comparative/complex, extracts key terms, recommends initial search mode

2. **evaluate_all_modes** â€” Runs all 3 search modes in parallel, generates answers for each using Gemini (or extractive fallback)

3. **select_best_mode** â€” Scores each answer using the RAGEvaluator (same formula as the dashboard so they always agree). Tie-breaks by answer relevance, then speed

4. **evaluate_quality** â€” Checks if the winner meets the quality threshold (0.82). If not and iterations remain, refines the query and retries

5. **finalize_answer** â€” Returns the best answer with full metadata: confidence, search mode used, agent reasoning, workflow path

**Key design:** The agent and the evaluation dashboard share a **single RAGEvaluator instance**, so the winner the agent picks and the winner shown in the dashboard are always consistent.

---

### ğŸ“Š Evaluation System

Four layers of evaluation, each answering a different question:

#### Layer 1: Embedding Metrics (always available, no API)
| Metric | What it measures |
|--------|-----------------|
| **Answer Relevance** | Cosine similarity between question and answer embeddings |
| **Context Precision** | % of retrieved chunks that are relevant (>0.3 threshold) |
| **Context Recall** | Coverage â€” did we retrieve enough relevant chunks? |
| **Faithfulness** | Similarity between answer and combined context |

#### Layer 2: LLM Judge (requires Gemini API)
Gemini reads the retrieved context and the answer, scores:
- **Faithfulness (0-1)** â€” Is every claim in the answer supported by the context?
- **Completeness (0-1)** â€” Does it fully address the question?
- **Reasoning** â€” One sentence explaining the score

The LLM judge score is blended with embedding metrics: `0.5Ã—embedding + 0.3Ã—faithfulness + 0.2Ã—completeness`

#### Layer 3: External Factual Check (requires Gemini API)
Completely independent of the document â€” Gemini checks: *"Based on your general knowledge, is this answer correct?"*

Returns: `correct` / `partially_correct` / `incorrect` / `unverifiable`

This is the key metric your manager asked about â€” it tells you whether the system is giving factually right answers, not just answers that match the document.

#### Layer 4: Consistency Testing
Runs the same question 3 times, measures:
- Score variance (std dev)
- Answer-level similarity across runs
- Consistency score: `1 - normalised_std_dev`

> **Note:** Consistency will always show 1.000 when Gemini is unavailable (rate limited) because the extractive fallback is deterministic. Real variance appears when Gemini generates answers.

#### Winner Reasoning
After selecting a winner, the system explains *why* using:
- Question type classification (factual/conceptual/comparative/complex)
- Actual score margins between methods
- The strongest metric for the winning method
- LLM judge verdict

Example: *"Your question is **factual** in nature. Semantic search (Vector) excels at factual questions because it maps your question directly to the most relevant passage by meaning, not just keywords. It scored 85% overall â€” 8% ahead of HYBRID (77%). Its strongest metric was **Answer Relevance** at 85%."*

---

## ğŸ”Œ API Reference

### Upload
```http
POST /upload
Content-Type: multipart/form-data
Body: file=<pdf>

Response: {
  "success": true,
  "chunks_created": 119,
  "chunk_breakdown": {"text": 84, "ocr": 0, "tables": 35},
  "vision_used": false,
  "collection_size": 119
}
```

### Query
```http
POST /query
Body: {"question": "...", "search_mode": "hybrid", "top_k": 5}

Response: {
  "answer": "...",
  "sources": [...],
  "search_mode": "hybrid",
  "clip_used": false,
  "search_time": 0.045
}
```

### Agentic Query
```http
POST /query/agentic
Body: {"question": "...", "max_iterations": 2}

Response: {
  "answer": "...",
  "confidence": 0.88,
  "quality_score": 0.877,
  "metadata": {
    "final_search_mode": "vector",
    "iterations": 1,
    "agent_thoughts": [...],
    "workflow_path": ["analyze_query", "evaluate_all_modes", ...]
  }
}
```

### Evaluate
```http
POST /evaluate/single
Body: {"question": "..."}

Response: {
  "winner": {
    "overall": {"method": "vector", "score": 0.877, "reasoning": "..."},
    "fastest": {"method": "bm25", "time": 0.007},
    "most_relevant": {"method": "vector", "score": 0.877}
  },
  "factual_check": {
    "verdict": "correct",
    "factual_accuracy": 0.9,
    "external_context": "..."
  },
  "methods": {
    "vector": {"metrics": {...}, "llm_judge": {...}},
    "bm25":   {"metrics": {...}},
    "hybrid": {"metrics": {...}}
  }
}
```

### Consistency Test
```http
POST /evaluate/consistency
Body: {"question": "...", "n_runs": 3, "mode": "hybrid"}

Response: {
  "consistency_score": 0.98,
  "is_consistent": true,
  "std_score": 0.012,
  "scores": [0.87, 0.86, 0.88]
}
```

---

## ğŸ“ Project Structure

```
DocuMind-AI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                  # Flask API (agentic endpoints)
â”‚   â”œâ”€â”€ rag_engine.py           # Core RAG: ChromaDB + BM25 + CLIP + Gemini
â”‚   â”œâ”€â”€ multimodal_processor.py # PDF â†’ Vision AI + Tables + CLIP embeddings
â”‚   â”œâ”€â”€ document_processor.py   # Text chunking pipeline
â”‚   â”œâ”€â”€ evaluation_metrics.py   # LLM Judge + Factual Check + Consistency
â”‚   â”œâ”€â”€ agentic_rag.py          # LangGraph agent workflow
â”‚   â”œâ”€â”€ agent_state.py          # TypedDict state definition
â”‚   â”œâ”€â”€ agent_tools.py          # Query analyzer + Answer evaluator
â”‚   â”œâ”€â”€ test_evaluation.py      # CLI batch evaluation runner
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.js              # Full React UI
â”‚       â””â”€â”€ index.js
â”œâ”€â”€ .env.example                # Template â€” copy to .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

| Variable | Required | Description |
|----------|----------|-------------|
| `GEMINI_API_KEY` | **Recommended** | Free at [aistudio.google.com](https://aistudio.google.com). Powers vision, generation, and LLM judge |
| `HF_API_KEY` | Optional | Hugging Face â€” fallback generation if Gemini unavailable |
| `ANTHROPIC_API_KEY` | Optional | Claude fallback |
| `OPENAI_API_KEY` | Optional | GPT-4o fallback |

**Gemini Free Tier Limits:**
- 15 requests/minute
- 1,500 requests/day
- Resets daily at midnight Pacific Time

---

## ğŸªŸ Windows: Poppler Setup (for Vision Processing)

Vision processing requires Poppler to convert PDF pages to images:

1. Download from [oschwartz10612/poppler-windows](https://github.com/oschwartz10612/poppler-windows/releases)
2. Extract to `C:\Program Files\poppler\`
3. Add `C:\Program Files\poppler\Library\bin` to your system PATH
4. Restart your terminal and the Flask server

Without Poppler, the system still works â€” it just uses text extraction only (no chart/diagram understanding).

---

## ğŸ§© Integrating Into Your Own Project

The core components are modular and can be used independently:

```python
# Use just the RAG engine
from rag_engine import RAGEngine
engine = RAGEngine(collection_name="my_docs")
engine.add_documents(chunks)
result = engine.query("your question", search_mode="hybrid")

# Use just the evaluator
from evaluation_metrics import RAGEvaluator
evaluator = RAGEvaluator(rag_engine=engine)
result = evaluator.evaluate_query("your question")
print(result["winner"]["overall"]["reasoning"])

# Use just the multimodal processor
from multimodal_processor import MultimodalProcessor
processor = MultimodalProcessor()
result = processor.process_multimodal_document("doc.pdf")
# result["ocr_chunks"] â€” vision-described page chunks
# result["table_chunks"] â€” structured table chunks

# Use the agent standalone
from agentic_rag import AgenticRAG
agent = AgenticRAG(rag_engine=engine, evaluator=evaluator)
result = agent.query("complex question requiring multi-step reasoning")
```

---

## ğŸ“š Key Concepts to Learn From

| Concept | File | What to study |
|---------|------|--------------|
| RAG basics | `rag_engine.py` | `_vector_search`, `_bm25_search`, `_reciprocal_rank_fusion` |
| Multimodal embeddings | `multimodal_processor.py` | `VisionLLMClient`, `CLIPEncoder` |
| LangGraph agents | `agentic_rag.py` | `_build_graph`, node functions |
| LLM evaluation | `evaluation_metrics.py` | `LLMJudge`, `_determine_winner`, `_generate_winner_reasoning` |
| Query analysis | `agent_tools.py` | `QueryAnalyzer.analyze_query` |

---

## ğŸ™ Built With

- [ChromaDB](https://www.trychroma.com/) â€” Vector database
- [sentence-transformers](https://www.sbert.net/) â€” Embeddings + CLIP
- [LangGraph](https://langchain-ai.github.io/langgraph/) â€” Agent orchestration
- [Google Gemini](https://ai.google.dev/) â€” Vision, generation, evaluation
- [Camelot](https://camelot-py.readthedocs.io/) â€” PDF table extraction
- [Flask](https://flask.palletsprojects.com/) â€” API server
- [React](https://reactjs.org/) â€” Frontend

---

<div align="center">
Made by <a href="https://github.com/uXmii">uXmii</a> Â· If this helped you, give it a â­
</div>
